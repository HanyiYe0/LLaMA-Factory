05/06/2024 10:15:52 - INFO - transformers.tokenization_utils_base - loading file qwen.tiktoken from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/qwen.tiktoken

05/06/2024 10:15:52 - INFO - transformers.tokenization_utils_base - loading file added_tokens.json from cache at None

05/06/2024 10:15:52 - INFO - transformers.tokenization_utils_base - loading file special_tokens_map.json from cache at None

05/06/2024 10:15:52 - INFO - transformers.tokenization_utils_base - loading file tokenizer_config.json from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/tokenizer_config.json

05/06/2024 10:15:52 - INFO - transformers.tokenization_utils_base - loading file tokenizer.json from cache at None

05/06/2024 10:15:52 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>

05/06/2024 10:15:52 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>

05/06/2024 10:15:52 - INFO - llmtuner.data.loader - Loading dataset ImTheFatedVillainChaptersDataset.json...

05/06/2024 10:15:52 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.

05/06/2024 10:15:54 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/config.json

05/06/2024 10:15:54 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/config.json

05/06/2024 10:15:54 - INFO - transformers.configuration_utils - Model config QWenConfig {
  "_name_or_path": "Qwen/Qwen-1_8B",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "Qwen/Qwen-1_8B--configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "Qwen/Qwen-1_8B--modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.40.1",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 151936
}


05/06/2024 10:15:54 - INFO - transformers.modeling_utils - loading weights file model.safetensors from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/model.safetensors.index.json

05/06/2024 10:15:54 - INFO - transformers.modeling_utils - Instantiating QWenLMHeadModel model under default dtype torch.float32.

05/06/2024 10:15:54 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {}


05/06/2024 10:15:58 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing QWenLMHeadModel.


05/06/2024 10:15:58 - INFO - transformers.modeling_utils - All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.

05/06/2024 10:15:58 - INFO - transformers.generation.configuration_utils - loading configuration file generation_config.json from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/generation_config.json

05/06/2024 10:15:58 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "chat_format": "raw",
  "do_sample": true,
  "eos_token_id": 151643,
  "max_new_tokens": 512,
  "pad_token_id": 151643,
  "stop_words_ids": [
    [
      151643
    ]
  ],
  "top_k": 0,
  "top_p": 0.8
}


05/06/2024 10:15:59 - WARNING - llmtuner.model.utils.checkpointing - You are using the old GC format, some features (e.g. BAdam) will be invalid.

05/06/2024 10:15:59 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.

05/06/2024 10:15:59 - INFO - llmtuner.model.utils.attention - Using vanilla Attention implementation.

05/06/2024 10:15:59 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA

05/06/2024 10:15:59 - INFO - llmtuner.model.loader - trainable params: 1572864 || all params: 1838401536 || trainable%: 0.0856

05/06/2024 10:15:59 - INFO - transformers.trainer - You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.

05/06/2024 10:15:59 - INFO - transformers.trainer - ***** Running training *****

05/06/2024 10:15:59 - INFO - transformers.trainer -   Num examples = 741

05/06/2024 10:15:59 - INFO - transformers.trainer -   Num Epochs = 3

05/06/2024 10:15:59 - INFO - transformers.trainer -   Instantaneous batch size per device = 2

05/06/2024 10:15:59 - INFO - transformers.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 16

05/06/2024 10:15:59 - INFO - transformers.trainer -   Gradient Accumulation steps = 8

05/06/2024 10:15:59 - INFO - transformers.trainer -   Total optimization steps = 138

05/06/2024 10:15:59 - INFO - transformers.trainer -   Number of trainable parameters = 1,572,864

05/06/2024 10:28:14 - INFO - llmtuner.extras.callbacks - {'loss': 2.4451, 'learning_rate': 4.9838e-05, 'epoch': 0.11}

05/06/2024 10:41:36 - INFO - llmtuner.extras.callbacks - {'loss': 2.4132, 'learning_rate': 4.9355e-05, 'epoch': 0.22}

05/06/2024 10:54:34 - INFO - llmtuner.extras.callbacks - {'loss': 2.3620, 'learning_rate': 4.8557e-05, 'epoch': 0.32}

05/06/2024 11:07:36 - INFO - llmtuner.extras.callbacks - {'loss': 2.3824, 'learning_rate': 4.7453e-05, 'epoch': 0.43}

05/06/2024 11:23:48 - INFO - llmtuner.extras.callbacks - {'loss': 2.3225, 'learning_rate': 4.6059e-05, 'epoch': 0.54}

05/06/2024 11:41:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.2307, 'learning_rate': 4.4393e-05, 'epoch': 0.65}

05/06/2024 11:59:15 - INFO - llmtuner.extras.callbacks - {'loss': 2.2488, 'learning_rate': 4.2475e-05, 'epoch': 0.75}

05/06/2024 12:16:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.2202, 'learning_rate': 4.0332e-05, 'epoch': 0.86}

05/06/2024 12:35:03 - INFO - llmtuner.extras.callbacks - {'loss': 2.2479, 'learning_rate': 3.7990e-05, 'epoch': 0.97}

05/06/2024 12:57:08 - INFO - llmtuner.extras.callbacks - {'loss': 2.2066, 'learning_rate': 3.5479e-05, 'epoch': 1.08}

05/06/2024 13:21:03 - INFO - llmtuner.extras.callbacks - {'loss': 2.1815, 'learning_rate': 3.2834e-05, 'epoch': 1.19}

05/06/2024 13:45:26 - INFO - llmtuner.extras.callbacks - {'loss': 2.1624, 'learning_rate': 3.0086e-05, 'epoch': 1.29}

05/06/2024 14:09:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.1386, 'learning_rate': 2.7273e-05, 'epoch': 1.40}

05/06/2024 14:34:31 - INFO - llmtuner.extras.callbacks - {'loss': 2.1395, 'learning_rate': 2.4431e-05, 'epoch': 1.51}

05/06/2024 14:59:01 - INFO - llmtuner.extras.callbacks - {'loss': 2.1227, 'learning_rate': 2.1596e-05, 'epoch': 1.62}

05/06/2024 15:24:08 - INFO - llmtuner.extras.callbacks - {'loss': 2.1069, 'learning_rate': 1.8805e-05, 'epoch': 1.73}

05/06/2024 15:48:23 - INFO - llmtuner.extras.callbacks - {'loss': 2.0953, 'learning_rate': 1.6094e-05, 'epoch': 1.83}

05/06/2024 16:12:56 - INFO - llmtuner.extras.callbacks - {'loss': 2.0962, 'learning_rate': 1.3498e-05, 'epoch': 1.94}

05/06/2024 16:36:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.0750, 'learning_rate': 1.1052e-05, 'epoch': 2.05}

05/06/2024 17:02:13 - INFO - llmtuner.extras.callbacks - {'loss': 2.0587, 'learning_rate': 8.7854e-06, 'epoch': 2.16}

05/06/2024 17:02:13 - INFO - transformers.trainer - Saving model checkpoint to saves/Qwen-1.8B/lora/test7/checkpoint-100

05/06/2024 17:02:13 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/config.json

05/06/2024 17:02:13 - INFO - transformers.configuration_utils - Model config QWenConfig {
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "Qwen/Qwen-1_8B--configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "Qwen/Qwen-1_8B--modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.40.1",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 151936
}


05/06/2024 17:02:13 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Qwen-1.8B/lora/test7/checkpoint-100/tokenizer_config.json

05/06/2024 17:02:13 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Qwen-1.8B/lora/test7/checkpoint-100/special_tokens_map.json

05/06/2024 17:27:35 - INFO - llmtuner.extras.callbacks - {'loss': 2.1033, 'learning_rate': 6.7291e-06, 'epoch': 2.26}

05/06/2024 17:52:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.0394, 'learning_rate': 4.9092e-06, 'epoch': 2.37}

05/06/2024 18:18:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.1261, 'learning_rate': 3.3494e-06, 'epoch': 2.48}

05/06/2024 18:43:14 - INFO - llmtuner.extras.callbacks - {'loss': 2.0962, 'learning_rate': 2.0697e-06, 'epoch': 2.59}

05/06/2024 19:09:18 - INFO - llmtuner.extras.callbacks - {'loss': 2.1046, 'learning_rate': 1.0868e-06, 'epoch': 2.70}

05/06/2024 19:33:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.0922, 'learning_rate': 4.1346e-07, 'epoch': 2.80}

05/06/2024 19:59:13 - INFO - llmtuner.extras.callbacks - {'loss': 2.0918, 'learning_rate': 5.8281e-08, 'epoch': 2.91}

05/06/2024 20:13:51 - INFO - transformers.trainer - 

Training completed. Do not forget to share your model on huggingface.co/models =)



05/06/2024 20:13:51 - INFO - transformers.trainer - Saving model checkpoint to saves/Qwen-1.8B/lora/test7

05/06/2024 20:13:51 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at /Users/hanyiye/.cache/huggingface/hub/models--Qwen--Qwen-1_8B/snapshots/fa6e214ccbbc6a55235c26ef406355b6bfdf5eed/config.json

05/06/2024 20:13:51 - INFO - transformers.configuration_utils - Model config QWenConfig {
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "Qwen/Qwen-1_8B--configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "Qwen/Qwen-1_8B--modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.40.1",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 151936
}


05/06/2024 20:13:52 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Qwen-1.8B/lora/test7/tokenizer_config.json

05/06/2024 20:13:52 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Qwen-1.8B/lora/test7/special_tokens_map.json

05/06/2024 20:13:52 - INFO - transformers.modelcard - Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

